{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark-basics\n",
    "\n",
    "Playing around with Spark via the PySpark interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import Row, SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "Unzip the data at ˙``data/pagecounts-20160101-000000_parsed.out˙``.\n",
    "\n",
    "Each line of the dataset, delimited by a white space and contains the statistics for one Wikimedia page. The schema looks as follows:\n",
    "- Project code: The project identifier for each page.\n",
    "- Page title: A string containing the title of the page.\n",
    "- Page hits: Number of requests on the specific hour.\n",
    "- Page size: Size of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/pagecounts.zip\n",
      "  inflating: data/pagecounts-20160101-000000_parsed.out  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o data/pagecounts.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Spark and create a RDD fro the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/22 22:25:50 WARN Utils: Your hostname, mark-machine resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)\n",
      "21/09/22 22:25:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/09/22 22:25:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Spark context\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "#Create RDD from external Data source\n",
    "rdd_dirty = sc.textFile(\"data/pagecounts-20160101-000000_parsed.out\")\n",
    "type(rdd_dirty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_rdd(rdd):\n",
    "    \"\"\" Create a clean rdd. \"\"\"\n",
    "    # split each line into the columns seperated by the whitespace delimiter\n",
    "    rdd_tmp = rdd.map(lambda x: x.split(\" \"))\n",
    "    \n",
    "    # return dataframe\n",
    "    return rdd_tmp.map(lambda x: Row(code=str(x[0]), title=str(x[1]), hits=int(x[2]), size=int(x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "# get dataframe from rdd\n",
    "rdd = get_clean_rdd(rdd=rdd_dirty)\n",
    "print(type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_n(rdd, n):\n",
    "    \"\"\" Take n from rdd. \"\"\"\n",
    "    return rdd.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(code='aa', title='271_a.C', hits=1, size=4675),\n",
       " Row(code='aa', title='Category:User_th', hits=1, size=4770),\n",
       " Row(code='aa', title='Chiron_Elias_Krase', hits=1, size=4694),\n",
       " Row(code='aa', title='Dassault_rafaele', hits=2, size=9372),\n",
       " Row(code='aa', title='E.Desv', hits=1, size=4662),\n",
       " Row(code='aa', title='File:Wiktionary-logo-en.png', hits=1, size=10752),\n",
       " Row(code='aa', title='Indonesian_Wikipedia', hits=1, size=4679),\n",
       " Row(code='aa', title='Main_Page', hits=5, size=266946),\n",
       " Row(code='aa', title='Requests_for_new_languages/Wikipedia_Banyumasan', hits=1, size=4733),\n",
       " Row(code='aa', title='Special:Contributions/203.144.160.245', hits=1, size=5812),\n",
       " Row(code='aa', title='Special:Contributions/5.232.61.79', hits=1, size=5805),\n",
       " Row(code='aa', title='Special:Contributions/Ayarportugal', hits=1, size=5808),\n",
       " Row(code='aa', title='Special:Contributions/Born2bgratis', hits=1, size=5812),\n",
       " Row(code='aa', title='Special:ListFiles/Betacommand', hits=1, size=5035),\n",
       " Row(code='aa', title='Special:ListFiles/Bohdan_p', hits=1, size=5036)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the first 15 records and print out the result.\n",
    "take_n(rdd, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_records(rdd):\n",
    "    \"\"\" Count the records in the rdd. \"\"\"\n",
    "    return rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3324129 records in the dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Determine the number of records the dataset has in total.\n",
    "print(f\"There are {count_records(rdd)} records in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_stats_col(rdd, col_name):\n",
    "    \"\"\" Computes the min, max, and average (mean) of a column in an rdd.\"\"\"\n",
    "    assert col_name in [\"hits\", \"size\"]\n",
    "    col = rdd.map(lambda x: x[col_name])\n",
    "    return {\n",
    "        \"min\": col.min(),\n",
    "        \"max\": col.max(),\n",
    "        \"mean\": col.mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min size = 0, max size = 141180155987, mean size = 132239.5695744666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the min, max, and average page size.\n",
    "col_name=\"size\"\n",
    "summary_stats = summary_stats_col(rdd=rdd, col_name=col_name)\n",
    "print(f\"min {col_name} = {summary_stats['min']}, \"\n",
    "      f\"max {col_name} = {summary_stats['max']}, \"\n",
    "      f\"mean {col_name} = {summary_stats['mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(code='en.mw', title='en', hits=5466346, size=141180155987)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the record(s) with the largest page size. If multiple records have the same size, list all of them.\n",
    "col_name = \"size\"\n",
    "assert col_name in [\"hits\", \"size\"]\n",
    "\n",
    "mymax = rdd.map(lambda x: x[col_name]).max()\n",
    "recs = rdd.filter(lambda x: mymax <= x[col_name])\n",
    "recs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
